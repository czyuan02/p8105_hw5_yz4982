---
title: "yz4982_HW5"
output:
  github_document: default
---

Problem 1
------------------------------------------------------------
```{r setup, include=FALSE}
library(tidyverse)
library(lubridate)
library(broom)
library(scales)
```

### Birthday paradox 
For group sizes n = 2, 3, ..., 50, simulate 10000 groups of n birthdays, check if any duplicates exist, and estimate  P(at least one shared birthday) by the sample mean. Then plot probability vs group size and comment.

### Function to test for duplicate birthdays
```{r}
set.seed(20251106)

has_duplicate_bday <- function(n) {
  # Draw n birthdays 1 to 365 with replacement
  bdays <- sample(1:365, size = n, replace = TRUE)
  length(unique(bdays)) < n
}

# Quick checks
has_duplicate_bday(2)
has_duplicate_bday(50)
```

### Run 10,000 simulations per n = 2..50
```{r}
sim_tbl <- map_dfr(2:50, \(n) {
  tibble(
    n = n,
    dup = replicate(10000, has_duplicate_bday(n))
  )
}) |>
  group_by(n) |>
  summarise(prob_shared = mean(dup), .groups = "drop")

head(sim_tbl, 10)
```

### Plot probability of at least one shared birthday vs group size
```{r}
p1 <- ggplot(sim_tbl, aes(x = n, y = prob_shared)) +
  geom_line(linewidth = 1) +
  geom_point() +
  geom_hline(yintercept = 0.5, linetype = "dashed") +
  labs(
    title = "Birthday paradox via simulation",
    x = "Group size",
    y = "P(at least one shared birthday)"
  ) +
  scale_y_continuous(labels = percent_format(accuracy = 1)) +
  scale_x_continuous(breaks = seq(0, 50, by = 5)) +
  theme_minimal(base_size = 14)

ggsave("figs/simulation.png", p1, width = 7, height = 5, dpi = 300)
```

Using 10,000 simulations per group size n = 2 to 50 with birthdays drawn uniformly from 1–365, the probability of at least one shared birthday rises quickly as n increases. It crosses 50% around n = 23 and reaches = 97% by n = 50. Thus, even moderately sized groups make shared birthdays very likely under these assumptions.

Problem 2
------------------------------------------------------------
### Design parameters
```{r}
set.seed(20251106)

# Sample size 
n <- 30
# True SD
sigma <- 5
# Test size
alpha <- 0.05
# True means to try
mus   <- 0:6
# Number of datasets per mu
reps  <- 500
```

Run one simulation for a given true mean mu
```{r}

sim_one <- function(mu) {
  x  <- rnorm(n, mean = mu, sd = sigma)
  tt <- t.test(x, mu = 0)
  td <- broom::tidy(tt)
  tibble(
    mu       = mu,
    estimate = td$estimate,
    pval     = td$p.value
  )
}
```

Repeat for each mu (0..6), with 5000 datasets each
```{r}
sim_res <- map_dfr(mus, function(mu) {
  map_dfr(1:reps, ~ sim_one(mu))
})
```

Summaries by mu
```{r}
sum_tbl <- sim_res |>
  group_by(mu) |>
  summarize(
    power            = mean(pval < alpha),
    mean_hat_overall = mean(estimate),
    mean_hat_reject  = mean(estimate[pval < alpha]),
    .groups = "drop"
  )
```

### Plot 1 power vs true mean 
```{r}
p_power <- ggplot(sum_tbl, aes(x = mu, y = power)) +
  geom_line(linewidth = 1) +
  geom_point(size = 2) +
  scale_y_continuous(labels = percent_format(accuracy = 1), limits = c(0, 1)) +
  scale_x_continuous(breaks = mus) +
  labs(
    title = "Power of one-sample t-test (n = 30, sd = 5, alpha = 0.05)",
    x = expression(true~mu),
    y = "Power (Pr(reject H[0]))"
  ) +
  theme_minimal(base_size = 14)

p_power

ggsave("figs/power_vs_mu.png",  p_power, width = 7, height = 5, dpi = 300)
```
As the true mean μ moves away from 0, the effect becomes larger, the power (the proportion of times H0 is rejected) monotonically increases.
When μ = 0, the power is 0.05, equal to the significance level.
When μ is around 2, it is approaching half.
When μ ≥ 3, it rapidly approaches 1.
The larger the effect, the more the sample mean's distribution deviates from zero, making it easier to cross the threshold of the t-test, and thus more likely to be significant.

### Plot 2 Average hat mu vs true mean
```{r}
plot_df <- sum_tbl |>
  select(mu,
         `All samples`   = mean_hat_overall,
         `Rejected only` = mean_hat_reject) |>
  pivot_longer(-mu, names_to = "type", values_to = "avg_hat")

p_means <- ggplot(plot_df, aes(x = mu, y = avg_hat, linetype = type)) +
  geom_line(linewidth = 1) +
  geom_point(size = 2) +
  # identity line y = x to judge closeness to the truth
  geom_abline(slope = 1, intercept = 0, linetype = "dotted") +
  scale_x_continuous(breaks = mus) +
  labs(
    title = expression(paste("Average ", hat(mu), " vs true ", mu,
                             " (overall vs rejected-only)")),
    x = expression(true~mu),
    y = expression(average~hat(mu)),
    linetype = NULL
  ) +
  theme_minimal(base_size = 14) +
  theme(legend.position = "top")

p_means

ggsave("figs/meanhat_vs_mu.png", p_means, width = 7, height = 5, dpi = 300)
```
The curves of all samples are basically close to y = x, which indicates that the sample mean ̂μ is approximately unbiased with respect to μ. 
Considering only the significant samples, the curve is above y = x when μ >0, not equal to the true μ. 
Reason: By filtering based on "whether significant", it is equivalent to selecting "more extreme" samples from both ends of the sampling distribution, resulting in selection bias (winner's curse), leading to E[μ hat ∣reject] > μ, when μ > 0. 
When μ is very large and the power is close to 1, almost all samples will be rejected, and the conditional filtering effect disappears, and the two curves will once again approach y = x. 
So the average of only considering significant results μ hat does not equal the true μ, unless the power is approximately equal to 1, because the significance screening will systematically amplify the estimated value.

Problem 3
------------------------------------------------------------
### Import data
```{r}
path <- "data/homicide-data.csv"
homi_raw <- readr::read_csv(path, show_col_types = FALSE)

dim(homi_raw) 
names(homi_raw)
glimpse(homi_raw) 
```
Each row is one homicide case in a large U.S. city 52,179 rows, 12 columns, with fields for reported date, victim name/age/sex/race, city/state, latitude/longitude, and case disposition.

### Generate "city_state" and summarize the "total number / number of unsolved cases" at the city level.
```{r}
homi <- homi_raw |>
  mutate(
    city_state = paste(city, state, sep = ", "),
    unsolved   = disposition %in% c("Closed without arrest", "Open/No arrest")
  )

city_summary <- homi |>
  group_by(city_state) |>
  summarise(
    total    = n(),
    unsolved = sum(unsolved),
    .groups  = "drop"
  ) |>
  arrange(desc(total))

head(city_summary, 10)
```

Extract the estimated values and confidence intervals
```{r}
balt <- city_summary |> filter(city_state == "Baltimore, MD")

# Number of successes = Number of unsolved cases
balt_test <- prop.test(x = balt$unsolved, n = balt$total)
balt_tidy <- broom::tidy(balt_test) |>
  transmute(
    city_state = "Baltimore, MD",
    estimate, conf.low, conf.high
  )

balt_tidy
```

```{r}
city_results <- city_summary |>
  mutate(
    test = purrr::map2(unsolved, total, ~ prop.test(.x, .y)),
    tidy = purrr::map(test, broom::tidy)
  ) |>
  select(city_state, tidy) |>
  unnest(tidy) |>
  select(city_state, estimate, conf.low, conf.high) |>
  arrange(estimate)

head(city_results)
```

The proportion of unsolved cases in each city and the 95% confidence interval
```{r}
city_results |>
  mutate(city_state = forcats::fct_reorder(city_state, estimate)) |>
  ggplot(aes(x = city_state, y = estimate)) +
  geom_point() +
  geom_errorbar(aes(ymin = conf.low, ymax = conf.high), width = 0.15) +
  coord_flip() +
  scale_y_continuous(labels = percent_format(accuracy = 1)) +
  labs(
    title = "Estimated proportion of unsolved homicides by city (95% CI)",
    x = NULL, y = "Proportion unsolved"
  ) +
  theme_minimal(base_size = 14)
```