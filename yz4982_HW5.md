yz4982_HW5
================

## Problem 1

### Birthday paradox

For group sizes `n = 2, 3, ..., 50`, simulate 10000 groups of `n`
birthdays, check if any duplicates exist, and estimate
$P(\text{at least one shared birthday})$ by the sample mean. Then plot
probability vs. group size and comment.

### Function to test for duplicate birthdays

``` r
set.seed(20251106)

has_duplicate_bday <- function(n) {
  # Draw n birthdays 1 to 365 with replacement
  bdays <- sample(1:365, size = n, replace = TRUE)
  length(unique(bdays)) < n
}

# Quick checks
has_duplicate_bday(2)
```

    ## [1] FALSE

``` r
has_duplicate_bday(50)
```

    ## [1] TRUE

### Run 10,000 simulations per n = 2..50

``` r
sim_tbl <- map_dfr(2:50, \(n) {
  tibble(
    n = n,
    dup = replicate(10000, has_duplicate_bday(n))
  )
}) |>
  group_by(n) |>
  summarise(prob_shared = mean(dup), .groups = "drop")

head(sim_tbl, 10)
```

    ## # A tibble: 10 × 2
    ##        n prob_shared
    ##    <int>       <dbl>
    ##  1     2      0.0027
    ##  2     3      0.008 
    ##  3     4      0.0152
    ##  4     5      0.0234
    ##  5     6      0.0397
    ##  6     7      0.0538
    ##  7     8      0.0779
    ##  8     9      0.089 
    ##  9    10      0.118 
    ## 10    11      0.134

### Plot probability of at least one shared birthday vs group size

``` r
p1 <- ggplot(sim_tbl, aes(x = n, y = prob_shared)) +
  geom_line(linewidth = 1) +
  geom_point() +
  geom_hline(yintercept = 0.5, linetype = "dashed") +
  labs(
    title = "Birthday paradox via simulation",
    x = "Group size",
    y = "P(at least one shared birthday)"
  ) +
  scale_y_continuous(labels = percent_format(accuracy = 1)) +
  scale_x_continuous(breaks = seq(0, 50, by = 5)) +
  theme_minimal(base_size = 14)

ggsave("figs/simulation.png", p1, width = 7, height = 5, dpi = 300)
```

Using 10,000 simulations per group size n = 2 to 50 with birthdays drawn
uniformly from 1–365, the probability of at least one shared birthday
rises quickly as n increases. It crosses 50% around n = 23 and reaches =
97% by n = 50. Thus, even moderately sized groups make shared birthdays
very likely under these assumptions.

## Problem 2

### Design parameters

``` r
set.seed(20251106)

# Sample size 
n <- 30
# True SD
sigma <- 5
# Test size
alpha <- 0.05
# True means to try
mus   <- 0:6
# Number of datasets per mu
reps  <- 500
```

Run one simulation for a given true mean mu

``` r
sim_one <- function(mu) {
  x  <- rnorm(n, mean = mu, sd = sigma)
  tt <- t.test(x, mu = 0)              # default is two-sided
  td <- broom::tidy(tt)
  tibble(
    mu       = mu,
    estimate = td$estimate,            # sample mean
    pval     = td$p.value
  )
}
```

Repeat for each mu (0..6), with 5000 datasets each

``` r
sim_res <- map_dfr(mus, function(mu) {
  map_dfr(1:reps, ~ sim_one(mu))
})
```

Summaries by mu

``` r
sum_tbl <- sim_res |>
  group_by(mu) |>
  summarize(
    power            = mean(pval < alpha),
    mean_hat_overall = mean(estimate),
    mean_hat_reject  = mean(estimate[pval < alpha]),
    .groups = "drop"
  )
```

Plot 1 Power vs true mean

``` r
p_power <- ggplot(sum_tbl, aes(x = mu, y = power)) +
  geom_line(linewidth = 1) +
  geom_point(size = 2) +
  scale_y_continuous(labels = percent_format(accuracy = 1), limits = c(0, 1)) +
  scale_x_continuous(breaks = mus) +
  labs(
    title = "Power of one-sample t-test (n = 30, sd = 5, alpha = 0.05)",
    x = expression(true~mu),
    y = "Power (Pr(reject H[0]))"
  ) +
  theme_minimal(base_size = 14)

p_power
```

![](yz4982_HW5_files/figure-gfm/unnamed-chunk-8-1.png)<!-- -->

``` r
ggsave("figs/power_vs_mu.png",  p_power, width = 7, height = 5, dpi = 300)
```

Plot 2 Average hat mu vs true mean

``` r
plot_df <- sum_tbl |>
  select(mu,
         `All samples`   = mean_hat_overall,
         `Rejected only` = mean_hat_reject) |>
  pivot_longer(-mu, names_to = "type", values_to = "avg_hat")

p_means <- ggplot(plot_df, aes(x = mu, y = avg_hat, linetype = type)) +
  geom_line(linewidth = 1) +
  geom_point(size = 2) +
  # identity line y = x to judge closeness to the truth
  geom_abline(slope = 1, intercept = 0, linetype = "dotted") +
  scale_x_continuous(breaks = mus) +
  labs(
    title = expression(paste("Average ", hat(mu), " vs true ", mu,
                             " (overall vs rejected-only)")),
    x = expression(true~mu),
    y = expression(average~hat(mu)),
    linetype = NULL
  ) +
  theme_minimal(base_size = 14) +
  theme(legend.position = "top")

p_means
```

![](yz4982_HW5_files/figure-gfm/unnamed-chunk-9-1.png)<!-- -->

``` r
ggsave("figs/meanhat_vs_mu.png", p_means, width = 7, height = 5, dpi = 300)
```

## Problem 3

### 

``` r
url <- "https://raw.githubusercontent.com/washingtonpost/data-homicides/master/homicide-data.csv"
hom <- readr::read_csv(url, show_col_types = FALSE)

glimpse(hom)
```

    ## Rows: 52,179
    ## Columns: 12
    ## $ uid           <chr> "Alb-000001", "Alb-000002", "Alb-000003", "Alb-000004", …
    ## $ reported_date <dbl> 20100504, 20100216, 20100601, 20100101, 20100102, 201001…
    ## $ victim_last   <chr> "GARCIA", "MONTOYA", "SATTERFIELD", "MENDIOLA", "MULA", …
    ## $ victim_first  <chr> "JUAN", "CAMERON", "VIVIANA", "CARLOS", "VIVIAN", "GERAL…
    ## $ victim_race   <chr> "Hispanic", "Hispanic", "White", "Hispanic", "White", "W…
    ## $ victim_age    <chr> "78", "17", "15", "32", "72", "91", "52", "52", "56", "4…
    ## $ victim_sex    <chr> "Male", "Male", "Female", "Male", "Female", "Female", "M…
    ## $ city          <chr> "Albuquerque", "Albuquerque", "Albuquerque", "Albuquerqu…
    ## $ state         <chr> "NM", "NM", "NM", "NM", "NM", "NM", "NM", "NM", "NM", "N…
    ## $ lat           <dbl> 35.09579, 35.05681, 35.08609, 35.07849, 35.13036, 35.151…
    ## $ lon           <dbl> -106.5386, -106.7153, -106.6956, -106.5561, -106.5810, -…
    ## $ disposition   <chr> "Closed without arrest", "Closed by arrest", "Closed wit…
